{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrlkwakxcd5q"
      },
      "source": [
        "# Lab3 - Assignment Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGQjg-wvcd5r"
      },
      "source": [
        "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvM68FWccd5s"
      },
      "source": [
        "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
        "\n",
        "The aims of the assignment are:\n",
        "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
        "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
        "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
        "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
        "* Learn how to evaluate the results qualitatively (by examining the data)\n",
        "* Get insight into differences between the two applied methods\n",
        "* Get insight into the effects of using linguistic preprocessing\n",
        "* Be able to describe differences between the two methods in terms of their results\n",
        "* Get insight into issues when applying these methods across different  domains\n",
        "\n",
        "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
        "\n",
        "We recommend you go through the notebooks in the following order:\n",
        "* **Read the assignment (see below)**\n",
        "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
        "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
        "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
        "\n",
        "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
        "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
        "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twu983mxcd5t"
      },
      "source": [
        "## Credits\n",
        "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6mZYRi9cd5t"
      },
      "source": [
        "## Part I: VADER assignments\n",
        "\n",
        "\n",
        "### Preparation (nothing to submit):\n",
        "To be able to answer the VADER questions you need to know how the tool works.\n",
        "* Read more about the VADER tool in [this blog](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/).  \n",
        "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
        "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
        "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt)\n",
        "\n",
        "\n",
        "### [3.5 points] Question1:\n",
        "\n",
        "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced.\n",
        "\n",
        "```\n",
        "INPUT SENTENCE 1 I love apples\n",
        "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
        "\n",
        "INPUT SENTENCE 2 I don't love apples\n",
        "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
        "\n",
        "INPUT SENTENCE 3 I love apples :-)\n",
        "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
        "\n",
        "INPUT SENTENCE 4 These houses are ruins\n",
        "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
        "\n",
        "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
        "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
        "\n",
        "INPUT SENTENCE 6 He lies in the chair in the garden\n",
        "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
        "\n",
        "INPUT SENTENCE 7 This house is like any house\n",
        "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We find that the first sentence is valid. The sentence \"I love apples\" is a positive sentence and the compound score should reflect the positivity exhibited in the sentence. In this case, since the compound score is 0.6369 (a high positive score), we can conclude that the sentence is positive.\n",
        "\n",
        "2. The sentence \"I don't love apples\" is the opposite of the first sentence and thus is a negative sentence. In this case, we expect VEDAR to produce a high negative score for the sentence, which is reflected in the -0.5216 (high negative score) score.\n",
        "\n",
        "3. \"I love apples :-)\" is similar to the first sentence. While it is arguable whether in this sentence the extra smiley face really increases the positivity of the sentence, we at least expect the score to be as positive as the first sentence since the same wording is used. We see that the score for this sentence is 0.7579, which is higher than the 1st sentence's observed score of 0.6369.\n",
        "\n",
        "4. \"These houses are in ruins\" is a sentence with a negative sentiment. We expect a high negative score from these sentences, which should distinguish them from the neutral score. With a VADER score of -0.4404, we find that the sentence is indeed classified as a negative sentence with a high negative compound score.\n",
        "\n",
        "5. \"These houses are certainly not considered ruins\". We would consider this sentence to be a neutral sentence. If we compare it to the first sentence: \"I love apples\" we see that the score difference between the 2 is only about 0.0502, but we can tell that the first sentence is much more positive than the second one. From the VADAR output, we can see that the sentence receives a more neutral score than a positive one, but because the compound score is a \"weighted\" score, the words, which are considered positive, have much more \"weight\" and thus VADER gives this sentence a much more positive score than we would agree with. We believe this should be much closer to neutral (still a positive score, just not as positive) than VADER currently classifies it as.\n",
        "\n",
        "6. \"He lies in the chair in the garden\". This sentence is classified as a \"negative\" when the sentence should have been classified as a \"neutral\". VADER classified the sentence as \"negative\" because it did not distinguish between the two different versions of \"lies\": \"lies to a person\", meaning that you tell a false statement to another person that you might or might not know is not true, whilst \"lies on a bed\", meaning to sit/rest on top of a bed. In this case, VEDAR only considered the \"lies\" word to refer to the act of providing false information, thus the sentence was classified as a negative sentence, when, in fact, it should have been classified as a neutral sentence.\n",
        "\n",
        "7. \"This house is like any other house.\" The following sentence was classified as a positive sentence with a score of 0.3612 when it should have been classified as a neutral sentence. As seen in sentence 5, VADER gave a much higher neutral score to the sentence than a positive one, but because the compound score is \"weighted\", the compound score produced a much more positive result than expected. This is due to the relatively high \"weight\" of the word \"like\" which in this context means \"similar to\", while VEDAR considered the word meaning to be \"enjoy/prefer\". Thus the score became much more positive.\n",
        "\n",
        "From these examples, we can see that while judging if a word has a positive meaning or a negative one, VADER does not consider the surrounding context around a word."
      ],
      "metadata": {
        "id": "vwrw6TwSkieM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TYpA3Pvcd5u"
      },
      "source": [
        "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
        "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
        "\n",
        "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
        "For each tweet, you should insert:\n",
        "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
        "* the text of the tweet\n",
        "* the Tweet-URL\n",
        "\n",
        "from:\n",
        "```\n",
        "    \"1\": {\n",
        "        \"sentiment_label\": \"\",\n",
        "        \"text_of_tweet\": \"\",\n",
        "        \"tweet_url\": \"\",\n",
        "```\n",
        "to:\n",
        "```\n",
        "\"1\": {\n",
        "        \"sentiment_label\": \"positive\",\n",
        "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
        "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
        "    },\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBlbWyQhcd5u"
      },
      "source": [
        "You can load your tweets with human annotation in the following way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qWKcuSZcd5x"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_B3KDoWcd5y"
      },
      "outputs": [],
      "source": [
        "my_tweets = json.load(open('my_tweets.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2DngMN1cd5z",
        "outputId": "342319a2-3066-4296-9639-8c673b627ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 {'sentiment_label': 'negative', 'text_of_tweet': \"They don't want you to know what SIDS REALLY is.\", 'tweet_url': 'https://x.com/DiedSuddenly_/status/1917356956936986696'}\n"
          ]
        }
      ],
      "source": [
        "for id_, tweet_info in my_tweets.items():\n",
        "    print(id_, tweet_info)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g77Dsaecd54"
      },
      "source": [
        "### [5 points] Question 3:\n",
        "\n",
        "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point.\n",
        "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
        "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF_0vbYStlXG",
        "outputId": "9efe60c7-e083-43a5-b808-5cd58de48eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import vader\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader_model = SentimentIntensityAnalyzer()\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def run_vader(textual_unit,\n",
        "              lemmatize=False,\n",
        "              parts_of_speech_to_consider=None,\n",
        "              verbose=0):\n",
        "    \"\"\"\n",
        "    Run VADER on a sentence from spacy\n",
        "\n",
        "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
        "    (by looping over doc.sents)\n",
        "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
        "    :param set parts_of_speech_to_consider:\n",
        "    -None or empty set: all parts of speech are provided\n",
        "    -non-empty set: only these parts of speech are considered.\n",
        "    :param int verbose: if set to 1, information is printed\n",
        "    about input and output\n",
        "\n",
        "    :rtype: dict\n",
        "    :return: vader output dict\n",
        "    \"\"\"\n",
        "    doc = nlp(textual_unit)\n",
        "\n",
        "    input_to_vader = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "\n",
        "            to_add = token.text\n",
        "\n",
        "            if lemmatize:\n",
        "                to_add = token.lemma_\n",
        "\n",
        "                if to_add == '-PRON-':\n",
        "                    to_add = token.text\n",
        "\n",
        "            if parts_of_speech_to_consider:\n",
        "                if token.pos_ in parts_of_speech_to_consider:\n",
        "                    input_to_vader.append(to_add)\n",
        "            else:\n",
        "                input_to_vader.append(to_add)\n",
        "\n",
        "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
        "\n",
        "    if verbose >= 1:\n",
        "        print()\n",
        "        print('INPUT SENTENCE', sent)\n",
        "        print('INPUT TO VADER', input_to_vader)\n",
        "        print('VADER OUTPUT', scores)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "_azMfW3-tC3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQYlIQVAcd54"
      },
      "outputs": [],
      "source": [
        "def vader_output_to_label(vader_output):\n",
        "    \"\"\"\n",
        "    map vader output e.g.,\n",
        "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
        "    to one of the following values:\n",
        "    a) positive float -> 'positive'\n",
        "    b) 0.0 -> 'neutral'\n",
        "    c) negative float -> 'negative'\n",
        "\n",
        "    :param dict vader_output: output dict from vader\n",
        "\n",
        "    :rtype: str\n",
        "    :return: 'negative' | 'neutral' | 'positive'\n",
        "    \"\"\"\n",
        "    compound = vader_output['compound']\n",
        "\n",
        "    if compound < 0:\n",
        "        return 'negative'\n",
        "    elif compound == 0.0:\n",
        "        return 'neutral'\n",
        "    elif compound > 0.0:\n",
        "        return 'positive'\n",
        "\n",
        "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
        "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
        "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHTeMkT8cd55",
        "outputId": "cd33d815-917c-4272-bf7f-f8aeffffbdc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canada has fallen to a slimy banker.  Goodbye free speech and guns.  It’s a sad day for the good Canadians.:\n",
            " Predicted: positive\n",
            " Actual: negative\n",
            "\n",
            "Does mint chocolate chip ice cream taste good?:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "A Tree Frog demonstrates how it 'adheres' to surfaces like leaves (or glass):\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "Mmm... Boston Cream Croissants! 🥐 #recipe https://recipesbyclare.com/recipes/boston-cream-pie-croissants:\n",
            " Predicted: neutral\n",
            " Actual: positive\n",
            "\n",
            "🚨| Kimi Antonelli reportedly checked 5+7 on his calculator “just to be sure.”:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "What’s the most stunning restaurant or café you’ve ever been to?:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "Welcome to Country should not be performed at ANZAC Day services. It is disrespectful to our veterans and must stop. We are there to pay our respects to those who served our country and remember their sacrifices.:\n",
            " Predicted: positive\n",
            " Actual: negative\n",
            "\n",
            "Harold forgot his medal at home but at least brought everyone a jar of raspberry soup.:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "This farmer rescued an abandoned family of pigs:\n",
            " Predicted: negative\n",
            " Actual: positive\n",
            "\n",
            "These cops are pigs who have names.:\n",
            " Predicted: neutral\n",
            " Actual: negative\n",
            "\n",
            "Oh to be a cow eating pretty pink flowers off a tree:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "So many relationships and marriages could be fixed if people could put their pride and ego aside, apologize, and actually change their behaviour.:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "Signing a small number of books in Boston today….:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "If you think about it, the best books are really just long spells that turn you into a different person for the rest of your life.:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "Do you believe that we landed on the moon? Be honest. 🤷🏻‍♂️:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "You scroll many moon. Come. Sit by fire. Brain need rest now.:\n",
            " Predicted: negative\n",
            " Actual: neutral\n",
            "\n",
            "Would you be happy to own this property? Yes No:\n",
            " Predicted: positive\n",
            " Actual: neutral\n",
            "\n",
            "Only 1 in 4 men is actually happy:\n",
            " Predicted: positive\n",
            " Actual: negative\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.76      0.81        17\n",
            "     neutral       0.75      0.33      0.46        18\n",
            "    positive       0.48      0.87      0.62        15\n",
            "\n",
            "    accuracy                           0.64        50\n",
            "   macro avg       0.70      0.65      0.63        50\n",
            "weighted avg       0.71      0.64      0.63        50\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweets = []\n",
        "all_vader_output = []\n",
        "gold = []\n",
        "\n",
        "data = {'negative': [], 'positive': [], 'neutral': []}\n",
        "\n",
        "# settings (to change for different experiments)\n",
        "to_lemmatize = True\n",
        "pos = set()\n",
        "\n",
        "for id_, tweet_info in my_tweets.items():\n",
        "    the_tweet = tweet_info['text_of_tweet']\n",
        "    vader_output = run_vader(the_tweet) # run vader\n",
        "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
        "\n",
        "    tweets.append(the_tweet)\n",
        "    all_vader_output.append(vader_label)\n",
        "    gold.append(tweet_info['sentiment_label'])\n",
        "    if tweet_info['sentiment_label'] != vader_label:\n",
        "      print(f\"{tweets[-1]}:\\n Predicted: {all_vader_output[-1]}\\n Actual: {gold[-1]}\\n\")\n",
        "      if tweet_info['sentiment_label'] == 'negative':\n",
        "        data['negative'].append([vader_label, tweets[-1], vader_output])\n",
        "      elif tweet_info['sentiment_label'] == 'positive':\n",
        "        data['positive'].append([vader_label, tweets[-1], vader_output])\n",
        "      else:\n",
        "        data['neutral'].append([vader_label, tweets[-1], vader_output])\n",
        "\n",
        "# use scikit-learn's classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(gold, all_vader_output))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Negative:\")\n",
        "for tweet_data in data['negative'][:(10 if len(data['negative']) > 10 else len(data))]: # <-- The fact that you can do this is both amazing and gives me a headache at the thought of trying to read this is in an actual codebase...\n",
        "  print(f\"Tweet: {tweet_data[1]}\")\n",
        "  print(f\"Precidted: {tweet_data[0]}\")\n",
        "  print(f\"Scores: {tweet_data[2]}\")\n",
        "\n",
        "print(\"\\nPositive:\")\n",
        "for tweet_data in data['positive'][:(10 if len(data['positive']) > 10 else len(data))]:\n",
        "  print(f\"Tweet: {tweet_data[1]}\")\n",
        "  print(f\"Precidted: {tweet_data[0]}\")\n",
        "  print(f\"Score: {tweet_data[2]}\")\n",
        "\n",
        "print(\"\\nNeutral:\")\n",
        "for tweet_data in data['neutral'][:(10 if len(data['neutral']) > 10 else len(data))]:\n",
        "  print(f\"Tweet: {tweet_data[1]}\")\n",
        "  print(f\"Precidted: {tweet_data[0]}\")\n",
        "  print(f\"Score: {tweet_data[2]}\")"
      ],
      "metadata": {
        "id": "g6drxS6MRlDt",
        "outputId": "f7c210b7-cb6b-40ff-bfa4-e3ed23c6282b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative:\n",
            "Tweet: Canada has fallen to a slimy banker.  Goodbye free speech and guns.  It’s a sad day for the good Canadians.\n",
            "Precidted: positive\n",
            "Scores: {'neg': 0.209, 'neu': 0.56, 'pos': 0.231, 'compound': 0.1531}\n",
            "Tweet: Welcome to Country should not be performed at ANZAC Day services. It is disrespectful to our veterans and must stop. We are there to pay our respects to those who served our country and remember their sacrifices.\n",
            "Precidted: positive\n",
            "Scores: {'neg': 0.086, 'neu': 0.788, 'pos': 0.126, 'compound': 0.4019}\n",
            "Tweet: These cops are pigs who have names.\n",
            "Precidted: neutral\n",
            "Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Positive:\n",
            "Tweet: Mmm... Boston Cream Croissants! 🥐 #recipe https://recipesbyclare.com/recipes/boston-cream-pie-croissants\n",
            "Precidted: neutral\n",
            "Score: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Tweet: This farmer rescued an abandoned family of pigs\n",
            "Precidted: negative\n",
            "Score: {'neg': 0.254, 'neu': 0.508, 'pos': 0.237, 'compound': -0.0516}\n",
            "\n",
            "Neutral:\n",
            "Tweet: Does mint chocolate chip ice cream taste good?\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'compound': 0.4404}\n",
            "Tweet: A Tree Frog demonstrates how it 'adheres' to surfaces like leaves (or glass)\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.815, 'pos': 0.185, 'compound': 0.3612}\n",
            "Tweet: 🚨| Kimi Antonelli reportedly checked 5+7 on his calculator “just to be sure.”\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.813, 'pos': 0.187, 'compound': 0.3182}\n",
            "Tweet: What’s the most stunning restaurant or café you’ve ever been to?\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.806, 'pos': 0.194, 'compound': 0.4391}\n",
            "Tweet: Harold forgot his medal at home but at least brought everyone a jar of raspberry soup.\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.2617}\n",
            "Tweet: Oh to be a cow eating pretty pink flowers off a tree\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.738, 'pos': 0.262, 'compound': 0.4939}\n",
            "Tweet: So many relationships and marriages could be fixed if people could put their pride and ego aside, apologize, and actually change their behaviour.\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.847, 'pos': 0.153, 'compound': 0.4215}\n",
            "Tweet: Signing a small number of books in Boston today….\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.0772}\n",
            "Tweet: If you think about it, the best books are really just long spells that turn you into a different person for the rest of your life.\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.6369}\n",
            "Tweet: Do you believe that we landed on the moon? Be honest. 🤷🏻‍♂️\n",
            "Precidted: positive\n",
            "Score: {'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.5106}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rruKqJhcd5-"
      },
      "source": [
        "### [4 points] Question 4:\n",
        "Run VADER on the set of airline tweets with the following settings:\n",
        "\n",
        "* Run VADER (as it is) on the set of airline tweets\n",
        "* Run VADER on the set of airline tweets after having lemmatized the text\n",
        "* Run VADER on the set of airline tweets with only adjectives\n",
        "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
        "* Run VADER on the set of airline tweets with only nouns\n",
        "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
        "* Run VADER on the set of airline tweets with only verbs\n",
        "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
        "\n",
        "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
        "* [3 points] b. Compare the scores and explain what they tell you.\n",
        "* - Does lemmatisation help? Explain why or why not.\n",
        "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap3M_sN9cd5_"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOxY4UyFcd5_"
      },
      "source": [
        "## Part II: scikit-learn assignments\n",
        "### [4 points] Question 5\n",
        "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
        "\n",
        "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
        "+ Train with different settings:\n",
        "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count')\n",
        "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10)\n",
        "* [1 point] a. Generate a classification_report for all experiments\n",
        "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ:\n",
        "    + which category performs best, is this the case for any setting?\n",
        "    + does the frequency threshold affect the scores? Why or why not according to you?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "iih6yN_UlZzA",
        "outputId": "081ad307-032b-4122-aa21-11e27fcdf5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkqp91-Jcd5_"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/airlinetweets.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/temp')"
      ],
      "metadata": {
        "id": "nZR4XaBRlQaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cwd = pathlib.Path.cwd()\n",
        "airline_tweets_folder = cwd.joinpath('temp/airlinetweets')\n",
        "print('path:', airline_tweets_folder)\n",
        "print('this will print True if the folder exists:',\n",
        "      airline_tweets_folder.exists())"
      ],
      "metadata": {
        "id": "VIF9ZOtlg6-S",
        "outputId": "6dba2085-1aed-4d0d-bb67-53a400d6a1ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path: /content/temp/airlinetweets\n",
            "this will print True if the folder exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = str(airline_tweets_folder)\n",
        "airline_tweets_train = load_files(path)"
      ],
      "metadata": {
        "id": "DTpuXHCbhOLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(airline_data, min_df: int, vectorization: str, return_vec = False):\n",
        "\n",
        "  docs_train, docs_test, y_train, y_test = train_test_split(airline_data.data, airline_data.target, test_size = 0.20)\n",
        "\n",
        "  representation = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
        "  docs_train = representation.fit_transform(docs_train)\n",
        "  docs_test = representation.transform(docs_test)\n",
        "\n",
        "  if vectorization == 'tfidf':\n",
        "    representation = TfidfTransformer()\n",
        "    docs_train = representation.fit_transform(docs_train)\n",
        "    docs_test = representation.transform(docs_test)\n",
        "\n",
        "  model = MultinomialNB().fit(docs_train, y_train)\n",
        "  if return_vec:\n",
        "    print('Returning Vectorizer')\n",
        "    return model, representation\n",
        "  return model, docs_test, y_test"
      ],
      "metadata": {
        "id": "6Qw0ZKps0Lip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(results):\n",
        "  for min_df in results:\n",
        "    for vectorization in results[min_df]:\n",
        "      print(f\"--- Results for min_df = {min_df}; Vectorization = {vectorization} ---\")\n",
        "      model, docs_test, y_test = results[min_df][vectorization]\n",
        "      model_pred = model.predict(docs_test)\n",
        "      print(classification_report(y_test, model_pred))\n",
        "    print('\\n')\n",
        "\n",
        "test_parameters = {\n",
        "    'vectorization': ['bag', 'tfidf'],\n",
        "    'min_df': [2, 5, 10],\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for min_df in test_parameters['min_df']:\n",
        "  results.update({min_df: {}})\n",
        "  for vectorization in test_parameters['vectorization']:\n",
        "    results[min_df].update({vectorization: train_classifier(airline_tweets_train, min_df, vectorization)})\n",
        "\n",
        "run_experiments(results)"
      ],
      "metadata": {
        "id": "ChKWPUvGdMhn",
        "outputId": "32a96a58-4875-46a4-a228-aa6cfc409e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Results for min_df = 2; Vectorization = bag ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88       356\n",
            "           1       0.88      0.71      0.79       295\n",
            "           2       0.82      0.88      0.85       300\n",
            "\n",
            "    accuracy                           0.84       951\n",
            "   macro avg       0.85      0.84      0.84       951\n",
            "weighted avg       0.85      0.84      0.84       951\n",
            "\n",
            "--- Results for min_df = 2; Vectorization = tfidf ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       350\n",
            "           1       0.83      0.67      0.74       299\n",
            "           2       0.83      0.85      0.84       302\n",
            "\n",
            "    accuracy                           0.82       951\n",
            "   macro avg       0.82      0.82      0.82       951\n",
            "weighted avg       0.82      0.82      0.82       951\n",
            "\n",
            "\n",
            "\n",
            "--- Results for min_df = 5; Vectorization = bag ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.94      0.87       322\n",
            "           1       0.87      0.72      0.79       319\n",
            "           2       0.88      0.87      0.88       310\n",
            "\n",
            "    accuracy                           0.85       951\n",
            "   macro avg       0.85      0.85      0.84       951\n",
            "weighted avg       0.85      0.85      0.84       951\n",
            "\n",
            "--- Results for min_df = 5; Vectorization = tfidf ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       348\n",
            "           1       0.84      0.71      0.77       302\n",
            "           2       0.86      0.85      0.85       301\n",
            "\n",
            "    accuracy                           0.83       951\n",
            "   macro avg       0.83      0.82      0.82       951\n",
            "weighted avg       0.83      0.83      0.82       951\n",
            "\n",
            "\n",
            "\n",
            "--- Results for min_df = 10; Vectorization = bag ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       368\n",
            "           1       0.73      0.77      0.75       277\n",
            "           2       0.86      0.77      0.82       306\n",
            "\n",
            "    accuracy                           0.82       951\n",
            "   macro avg       0.81      0.81      0.81       951\n",
            "weighted avg       0.82      0.82      0.82       951\n",
            "\n",
            "--- Results for min_df = 10; Vectorization = tfidf ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       360\n",
            "           1       0.78      0.74      0.76       294\n",
            "           2       0.83      0.80      0.82       297\n",
            "\n",
            "    accuracy                           0.81       951\n",
            "   macro avg       0.81      0.81      0.81       951\n",
            "weighted avg       0.81      0.81      0.81       951\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- By analyzing the results, we can find that the best combination of vectorization and min_df was a bag vectorization with a min_df value of 5. across the board, we find that bag vectorization outperformed the tfidf vectorization.\n",
        "\n",
        "- Looking at the results above, we can see that the frequency threshold does impact the accuracy of the model. The difference between a value of 2 and a value of 5 for this model was negligible. It is only when we increase the value too much, like with a value of 10, does the performance of our model noticeably decreases."
      ],
      "metadata": {
        "id": "QUjlFywatUAU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjaX2DsYcd6F"
      },
      "source": [
        "### [4 points] Question 6: Inspecting the best scoring features\n",
        "\n",
        "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
        "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
        "* [3 points] b. Look at the lists and consider the following issues:\n",
        "    + [1 point] Which features did you expect for each separate class and why?\n",
        "    + [1 point] Which features did you not expect and why ?\n",
        "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsdURW0Ocd6F",
        "outputId": "d1987d8e-0853-4bdb-cbae-2ba5041cf337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returning Vectorizer\n",
            "Important words in negative documents\n",
            "0 1499.0 @\n",
            "0 1377.0 united\n",
            "0 1197.0 .\n",
            "0 423.0 ``\n",
            "0 386.0 flight\n",
            "0 373.0 ?\n",
            "0 353.0 !\n",
            "0 303.0 #\n",
            "0 218.0 n't\n",
            "0 142.0 ''\n",
            "0 120.0 's\n",
            "0 110.0 service\n",
            "0 105.0 virginamerica\n",
            "0 100.0 :\n",
            "0 95.0 get\n",
            "0 91.0 delayed\n",
            "0 89.0 cancelled\n",
            "0 87.0 customer\n",
            "0 84.0 bag\n",
            "0 82.0 plane\n",
            "0 79.0 time\n",
            "0 78.0 ...\n",
            "0 74.0 'm\n",
            "0 73.0 -\n",
            "0 71.0 hours\n",
            "0 70.0 ;\n",
            "0 67.0 http\n",
            "0 64.0 airline\n",
            "0 64.0 &\n",
            "0 62.0 hour\n",
            "0 62.0 gate\n",
            "0 60.0 late\n",
            "0 59.0 still\n",
            "0 59.0 help\n",
            "0 56.0 would\n",
            "0 55.0 ca\n",
            "0 54.0 flights\n",
            "0 54.0 2\n",
            "0 53.0 amp\n",
            "0 52.0 worst\n",
            "0 52.0 one\n",
            "0 49.0 delay\n",
            "0 48.0 've\n",
            "0 46.0 back\n",
            "0 45.0 waiting\n",
            "0 45.0 $\n",
            "0 43.0 never\n",
            "0 43.0 like\n",
            "0 42.0 us\n",
            "0 42.0 flightled\n",
            "0 42.0 (\n",
            "0 41.0 lost\n",
            "0 40.0 ever\n",
            "0 39.0 day\n",
            "0 39.0 3\n",
            "0 39.0 )\n",
            "0 38.0 check\n",
            "0 38.0 bags\n",
            "0 36.0 seat\n",
            "0 36.0 really\n",
            "0 36.0 fly\n",
            "0 35.0 wait\n",
            "0 35.0 thanks\n",
            "0 35.0 people\n",
            "0 35.0 luggage\n",
            "0 33.0 u\n",
            "0 33.0 even\n",
            "0 33.0 due\n",
            "0 32.0 last\n",
            "0 32.0 hold\n",
            "0 32.0 crew\n",
            "0 32.0 4\n",
            "0 31.0 ticket\n",
            "0 30.0 could\n",
            "0 30.0 airport\n",
            "0 29.0 seats\n",
            "0 29.0 guys\n",
            "0 29.0 got\n",
            "0 29.0 call\n",
            "0 29.0 baggage\n",
            "-----------------------------------------\n",
            "Important words in neutral documents\n",
            "1 1411.0 @\n",
            "1 518.0 ?\n",
            "1 510.0 .\n",
            "1 308.0 jetblue\n",
            "1 279.0 :\n",
            "1 276.0 southwestair\n",
            "1 259.0 united\n",
            "1 244.0 #\n",
            "1 239.0 ``\n",
            "1 217.0 flight\n",
            "1 189.0 americanair\n",
            "1 181.0 !\n",
            "1 176.0 http\n",
            "1 155.0 usairways\n",
            "1 145.0 's\n",
            "1 88.0 get\n",
            "1 73.0 ''\n",
            "1 70.0 virginamerica\n",
            "1 67.0 -\n",
            "1 66.0 help\n",
            "1 65.0 please\n",
            "1 63.0 flights\n",
            "1 61.0 )\n",
            "1 52.0 n't\n",
            "1 50.0 need\n",
            "1 50.0 (\n",
            "1 47.0 ...\n",
            "1 46.0 dm\n",
            "1 44.0 ;\n",
            "1 40.0 us\n",
            "1 39.0 would\n",
            "1 39.0 &\n",
            "1 38.0 “\n",
            "1 38.0 'm\n",
            "1 37.0 fleet\n",
            "1 37.0 fleek\n",
            "1 35.0 ”\n",
            "1 35.0 way\n",
            "1 34.0 tomorrow\n",
            "1 34.0 thanks\n",
            "1 34.0 know\n",
            "1 31.0 flying\n",
            "1 31.0 cancelled\n",
            "1 30.0 change\n",
            "1 30.0 amp\n",
            "1 28.0 one\n",
            "1 28.0 number\n",
            "1 27.0 hi\n",
            "1 27.0 fly\n",
            "1 26.0 today\n",
            "1 26.0 check\n",
            "1 25.0 go\n",
            "1 25.0 could\n",
            "1 24.0 travel\n",
            "1 24.0 airport\n",
            "1 23.0 time\n",
            "1 23.0 like\n",
            "1 22.0 new\n",
            "1 22.0 destinationdragons\n",
            "1 21.0 trying\n",
            "1 21.0 ticket\n",
            "1 21.0 sent\n",
            "1 21.0 see\n",
            "1 21.0 next\n",
            "1 21.0 chance\n",
            "1 20.0 rt\n",
            "1 20.0 follow\n",
            "1 19.0 tickets\n",
            "1 19.0 guys\n",
            "1 18.0 reservation\n",
            "1 18.0 make\n",
            "1 18.0 back\n",
            "1 18.0 add\n",
            "1 18.0 2\n",
            "1 17.0 start\n",
            "1 17.0 question\n",
            "1 17.0 last\n",
            "1 17.0 going\n",
            "1 17.0 gate\n",
            "1 17.0 first\n",
            "-----------------------------------------\n",
            "Important words in positive documents\n",
            "2 1333.0 @\n",
            "2 1066.0 !\n",
            "2 782.0 .\n",
            "2 302.0 southwestair\n",
            "2 294.0 #\n",
            "2 291.0 thanks\n",
            "2 284.0 jetblue\n",
            "2 258.0 united\n",
            "2 251.0 thank\n",
            "2 231.0 ``\n",
            "2 179.0 flight\n",
            "2 176.0 americanair\n",
            "2 171.0 :\n",
            "2 138.0 great\n",
            "2 134.0 usairways\n",
            "2 90.0 service\n",
            "2 88.0 )\n",
            "2 82.0 virginamerica\n",
            "2 78.0 http\n",
            "2 72.0 love\n",
            "2 65.0 guys\n",
            "2 64.0 best\n",
            "2 63.0 customer\n",
            "2 63.0 ;\n",
            "2 58.0 awesome\n",
            "2 57.0 much\n",
            "2 57.0 's\n",
            "2 48.0 good\n",
            "2 47.0 got\n",
            "2 45.0 airline\n",
            "2 43.0 time\n",
            "2 43.0 -\n",
            "2 42.0 &\n",
            "2 41.0 n't\n",
            "2 40.0 get\n",
            "2 39.0 today\n",
            "2 38.0 us\n",
            "2 38.0 crew\n",
            "2 38.0 amazing\n",
            "2 36.0 fly\n",
            "2 34.0 made\n",
            "2 34.0 flying\n",
            "2 34.0 amp\n",
            "2 31.0 see\n",
            "2 31.0 help\n",
            "2 31.0 ''\n",
            "2 30.0 appreciate\n",
            "2 28.0 response\n",
            "2 28.0 gate\n",
            "2 27.0 work\n",
            "2 27.0 home\n",
            "2 27.0 ever\n",
            "2 27.0 (\n",
            "2 25.0 know\n",
            "2 24.0 nice\n",
            "2 24.0 flights\n",
            "2 24.0 first\n",
            "2 24.0 back\n",
            "2 24.0 ...\n",
            "2 24.0 'm\n",
            "2 24.0 'll\n",
            "2 23.0 tonight\n",
            "2 23.0 new\n",
            "2 23.0 helpful\n",
            "2 23.0 day\n",
            "2 23.0 ?\n",
            "2 23.0 're\n",
            "2 22.0 well\n",
            "2 22.0 southwest\n",
            "2 22.0 like\n",
            "2 21.0 u\n",
            "2 21.0 team\n",
            "2 21.0 job\n",
            "2 21.0 always\n",
            "2 20.0 yes\n",
            "2 20.0 would\n",
            "2 20.0 really\n",
            "2 19.0 plane\n",
            "2 19.0 one\n",
            "2 19.0 happy\n"
          ]
        }
      ],
      "source": [
        "def important_features_per_class(vectorizer,classifier,n=80):\n",
        "    class_labels = classifier.classes_\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
        "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
        "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
        "    print(\"Important words in negative documents\")\n",
        "    for coef, feat in topn_class1:\n",
        "        print(class_labels[0], coef, feat)\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(\"Important words in neutral documents\")\n",
        "    for coef, feat in topn_class2:\n",
        "        print(class_labels[1], coef, feat)\n",
        "    print(\"-----------------------------------------\")\n",
        "    print(\"Important words in positive documents\")\n",
        "    for coef, feat in topn_class3:\n",
        "        print(class_labels[2], coef, feat)\n",
        "\n",
        "test_parameters = {\n",
        "    'vectorization': ['bag'],\n",
        "    'min_df': [2],\n",
        "}\n",
        "\n",
        "for min_df in test_parameters['min_df']:\n",
        "  for vectorization in test_parameters['vectorization']:\n",
        "    results[min_df].update({vectorization: train_classifier(airline_tweets_train, min_df, vectorization, return_vec = True)})\n",
        "\n",
        "model, vectorizer = results[2]['bag']\n",
        "important_features_per_class(vectorizer=vectorizer, classifier=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the negative class, we expected words like flight, time, delay, service, cancelled, late, worst, never, luggage, and more to be present as the top-scoring words. This is due to the fact that when people complain about a problem with an airline, they usually complain about specific topics like baggage being lost or plane being late, thus it is no surprise that these words appear in our rankings.\n",
        "- For the positive class, we expected words like flight, great, thanks, love, best, awesome, thank, and more to appear. These words express general happiness about the flight, and unless you are ready to review, most people will express their positive experience about the flights using general language.\n",
        "- Neutral: help, please, need, and know would be used a lot since they are generally used in a variety of contexts and questions, which usually tend to stay neutral."
      ],
      "metadata": {
        "id": "utuFbTaJwTFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Negative: we did not expect to see words like \"like\" and \"thanks\" since they are usually associated with positives rather than negatives.\n",
        "\n",
        "- Positive: apart from some special symbols, most of the words here do fit our perception of what words might express positive sentiment.\n",
        "\n",
        "- Neutral: words like \"cancelled\" should probably be classified as a negative class, since that is one of the major problems that people complain about when flying - cancelled flights."
      ],
      "metadata": {
        "id": "f-xQiolAymzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trying to improve the model, we would first remove the names of the airlines since if we want to understand which airlines are the best using this model, we would need to take into account all of the tweets and their context and then evaluate the problems and the good sides of the airline (but if that is the goal, then the dataset should probably be something like review sites and not tweets since people usually don't post about positive experiences in flights unless it's something that happens rarely, like being upgraded to better seats). We would also remove punctuation since we are training the Bayesian model which is not the best at using context to judge the sentiment. If we were training a transformer model instead, we would need punctuation since it is really important in deciding the meaning of words, which the transformer takes into account when producing results."
      ],
      "metadata": {
        "id": "fkmNYgtj0up-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1C1OsVqcd6F"
      },
      "source": [
        "### [Optional! (will not  be graded)] Question 7\n",
        "Train the model on airline tweets and test it on your own set of tweets\n",
        "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
        "+ Apply the model on your own set of tweets and generate the classification report\n",
        "* [1 point] a. Carry out a quantitative analysis.\n",
        "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
        "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YqJgkBkcd6F"
      },
      "source": [
        "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
        "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings\n",
        "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
        "* [1 point] c. Discuss whether the model achieved what you expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwgbkmuPcd6L"
      },
      "source": [
        "## End of this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKu5-QJ4cd6L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}